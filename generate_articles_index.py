#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¨˜äº‹ä¸€è¦§ç”¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
generated_articles/å†…ã®å…¨è¨˜äº‹ã‚’è§£æã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
"""

import os
import json
import re
import glob
from datetime import datetime

def extract_metadata_from_html(html_path):
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
        title_match = re.search(r'<meta property="og:title" content="([^"]+)">', content)
        title = title_match.group(1) if title_match else "ç„¡é¡Œ"

        # èª¬æ˜æ–‡æŠ½å‡º
        desc_match = re.search(r'<meta name="description" content="([^"]+)">', content)
        description = desc_match.group(1) if desc_match else ""

        # ç”»åƒURLæŠ½å‡º
        image_match = re.search(r'<meta property="og:image" content="([^"]+)">', content)
        image = image_match.group(1) if image_match else ""

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼æŠ½å‡º
        category_match = re.search(r'<span class="category">([^<]+)</span>', content)
        category = category_match.group(1) if category_match else "å€Ÿé‡‘è¿”æ¸ˆ"

        # æ—¥ä»˜æŠ½å‡º
        date_match = re.search(r'<time datetime="([^"]+)">', content)
        date = date_match.group(1) if date_match else "2025-10-06"

        # ã„ã„ã­æ•°æŠ½å‡º
        likes_match = re.search(r'ğŸ‘ (\d+) ã„ã„ã­', content)
        likes = int(likes_match.group(1)) if likes_match else 0

        # TikTok URLæŠ½å‡º
        tiktok_match = re.search(r'href="(https://www\.tiktok\.com/@hensaikitsui/video/[^"]+)"', content)
        tiktok_url = tiktok_match.group(1) if tiktok_match else ""

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords_match = re.search(r'<meta name="keywords" content="([^"]+)">', content)
        keywords = keywords_match.group(1).split(',') if keywords_match else []

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰IDã‚’æŠ½å‡º
        filename = os.path.basename(html_path)
        article_id = filename.replace('.html', '')

        return {
            "id": article_id,
            "title": title.strip(),
            "description": description.strip(),
            "category": category.strip(),
            "date": date,
            "image": image,
            "likes": likes,
            "tiktok_url": tiktok_url,
            "keywords": [k.strip() for k in keywords],
            "url": f"generated_articles/{filename}"
        }

    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {html_path} - {str(e)}")
        return None


def generate_articles_index():
    """è¨˜äº‹ä¸€è¦§JSONã‚’ç”Ÿæˆ"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    articles_dir = os.path.join(base_dir, 'generated_articles')

    if not os.path.exists(articles_dir):
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {articles_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # å…¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    html_files = glob.glob(os.path.join(articles_dir, 'article-*.html'))

    print("=" * 70)
    print("è¨˜äº‹ä¸€è¦§JSONç”Ÿæˆ")
    print("=" * 70)
    print(f"\nå¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {articles_dir}")
    print(f"è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(html_files)}ä»¶\n")

    articles = []
    success_count = 0
    error_count = 0

    for html_path in sorted(html_files):
        metadata = extract_metadata_from_html(html_path)

        if metadata:
            articles.append(metadata)
            success_count += 1
            if success_count % 20 == 0:
                print(f"å‡¦ç†ä¸­... {success_count}/{len(html_files)}")
        else:
            error_count += 1

    # ã„ã„ã­æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    articles.sort(key=lambda x: x['likes'], reverse=True)

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    output_path = os.path.join(base_dir, 'articles-index.json')

    output_data = {
        "generated_at": datetime.now().isoformat(),
        "total_articles": len(articles),
        "articles": articles
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ å®Œäº†: {success_count}ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†")
    if error_count > 0:
        print(f"âš  ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")

    print("\n" + "=" * 70)
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"åˆè¨ˆè¨˜äº‹æ•°: {len(articles)}ä»¶")
    print("=" * 70)

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥çµ±è¨ˆ
    categories = {}
    for article in articles:
        cat = article['category']
        categories[cat] = categories.get(cat, 0) + 1

    print("\nã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥è¨˜äº‹æ•°ã€‘")
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        print(f"  {cat}: {count}ä»¶")

    print("\nã€äººæ°—è¨˜äº‹TOP5ã€‘")
    for i, article in enumerate(articles[:5], 1):
        print(f"  {i}. {article['title'][:50]}... ({article['likes']}ã„ã„ã­)")


if __name__ == '__main__':
    generate_articles_index()
